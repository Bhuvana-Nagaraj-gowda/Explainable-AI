{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "JtS38tg7jFm7",
        "outputId": "44e2e55e-3351-4358-f26a-aa1162d2586c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'stable_baselines3'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6df21af8f773>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_checker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, History\n",
        "from tensorflow.keras import regularizers\n",
        "try:\n",
        "    import shap\n",
        "except ImportError:\n",
        "    pass\n",
        "import gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "# === Load dataset ===\n",
        "df = pd.read_csv(\"Merged05.csv\")\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Identify target column (assuming it contains 'label', 'attack', or 'class')\n",
        "target_col = [col for col in df.columns if 'label' in col.lower() or 'attack' in col.lower() or 'class' in col.lower()][0]\n",
        "\n",
        "# === Separate label and features ===\n",
        "y = df[target_col]\n",
        "X = df.drop(columns=[target_col]).select_dtypes(include=[np.number])\n",
        "\n",
        "# Clean and align\n",
        "df_clean = pd.concat([X, y], axis=1).replace([np.inf, -np.inf], np.nan).dropna()\n",
        "X = df_clean.drop(columns=[target_col])\n",
        "y = df_clean[target_col]\n",
        "\n",
        "# Group rare classes (e.g., classes with < 100 samples)\n",
        "class_counts = y.value_counts()\n",
        "rare_classes = class_counts[class_counts < 100].index\n",
        "y_grouped = y.copy()\n",
        "for rare_class in rare_classes:\n",
        "    y_grouped[y_grouped == rare_class] = \"Rare\"\n",
        "print(\"Class distribution after grouping rare classes:\\n\", y_grouped.value_counts())\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y_grouped)\n",
        "num_classes = len(np.unique(y_encoded))\n",
        "\n",
        "# Feature selection using correlation\n",
        "corr_matrix = X.corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
        "X = X.drop(columns=to_drop)\n",
        "\n",
        "# Normalize and clip\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = np.clip(X_scaled, -5, 5)\n",
        "\n",
        "# Oversample rare classes with SMOTE\n",
        "smote = SMOTE(random_state=42, sampling_strategy='not majority', k_neighbors=5)\n",
        "X_scaled_resampled, y_encoded_resampled = smote.fit_resample(X_scaled, y_encoded)\n",
        "\n",
        "# 5-fold cross-validation split\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X_scaled_resampled, y_encoded_resampled, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)  # 10% of 80% for validation\n",
        "\n",
        "# Compute class weights with adjustments\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_encoded_resampled), y=y_encoded_resampled)\n",
        "class_weight_dict = {i: w for i, w in enumerate(class_weights)}  # Remove cap\n",
        "poor_classes = ['DDOS-SYN_FLOOD', 'MIRAI-GREIP_FLOOD', 'RECON-OSSCAN', 'RECON-PORTSCAN']\n",
        "for cls in poor_classes:\n",
        "    idx = label_encoder.transform([cls])[0]\n",
        "    if idx in class_weight_dict:\n",
        "        class_weight_dict[idx] *= 1.5\n",
        "print(\"Class weights:\\n\", class_weight_dict)\n",
        "\n",
        "# Debug: Print shapes\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "# === Focal Loss Implementation ===\n",
        "def focal_loss(gamma=3.0, alpha=0.25):\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "        y_true = tf.one_hot(y_true, depth=num_classes)\n",
        "        epsilon = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
        "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
        "        weight = alpha * y_true * tf.pow(1 - y_pred, gamma)\n",
        "        loss = weight * cross_entropy\n",
        "        return tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n",
        "    return focal_loss_fixed\n",
        "\n",
        "# === Enhanced Feedforward Neural Network ===\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(num_classes, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=focal_loss(gamma=3.0, alpha=0.25),\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# Train with history callback to track accuracy\n",
        "history = History()\n",
        "early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience=3, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.5, patience=2, min_lr=1e-7)\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_val, y_val),\n",
        "          callbacks=[early_stopping, lr_scheduler, history],\n",
        "          class_weight=class_weight_dict, verbose=1)\n",
        "\n",
        "# === Random Forest Ensemble ===\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=class_weight_dict)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Combine predictions (simple voting)\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred_nn = np.argmax(y_pred_probs, axis=1)\n",
        "ensemble_pred = np.round((y_pred_nn + rf_pred) / 2).astype(int)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Neural Network Classification Report:\\n\", classification_report(y_test, y_pred_nn, target_names=label_encoder.classes_, zero_division=0))\n",
        "print(\"Ensemble Classification Report:\\n\", classification_report(y_test, ensemble_pred, target_names=label_encoder.classes_, zero_division=0))\n",
        "\n",
        "# === Generate SHAP Plot ===\n",
        "if 'shap' in globals():\n",
        "    shap_input = X_test[:100]\n",
        "    print(\"SHAP input shape:\", shap_input.shape)\n",
        "    explainer = shap.KernelExplainer(lambda x: model.predict(x), shap_input)\n",
        "    shap_values = explainer.shap_values(shap_input, nsamples=100)\n",
        "    shap.summary_plot(shap_values, shap_input, feature_names=X.columns, plot_type=\"bar\", show=False)\n",
        "    plt.savefig(\"shap_summary.png\")\n",
        "    plt.close()\n",
        "    print(\"SHAP summary plot saved as shap_summary.png\")\n",
        "\n",
        "# === Generate Training Accuracy Graph ===\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"training_accuracy.png\")\n",
        "plt.close()\n",
        "print(\"Training accuracy graph saved as training_accuracy.png\")\n",
        "\n",
        "# === Generate Confusion Matrix Plot ===\n",
        "conf_matrix = confusion_matrix(y_test, ensemble_pred)\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.imshow(conf_matrix, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "plt.title(\"Ensemble Confusion Matrix\")\n",
        "plt.colorbar()\n",
        "plt.ylabel(\"True label\")\n",
        "plt.xlabel(\"Predicted label\")\n",
        "tick_marks = np.arange(num_classes)\n",
        "plt.xticks(tick_marks, label_encoder.classes_, rotation=45)\n",
        "plt.yticks(tick_marks, label_encoder.classes_)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"confusion_matrix_final.png\")\n",
        "plt.close()\n",
        "print(\"Confusion matrix plot saved as confusion_matrix_final.png\")\n",
        "\n",
        "# === Reinforcement Learning for Mitigation ===\n",
        "class IoTThreatMitigationEnv(gym.Env):\n",
        "    def __init__(self, model, rf_model, X_data, y_true):\n",
        "        super(IoTThreatMitigationEnv, self).__init__()\n",
        "        self.model = model\n",
        "        self.rf_model = rf_model\n",
        "        self.X_data = X_data\n",
        "        self.y_true = y_true\n",
        "        self.current_step = 0\n",
        "        self.action_space = gym.spaces.Discrete(3)  # 0: Block, 1: Throttle, 2: Allow\n",
        "        self.observation_space = gym.spaces.Box(low=-5, high=5, shape=(self.X_data.shape[1] + num_classes,), dtype=np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        X_sample = self.X_data[self.current_step]\n",
        "        y_pred_nn = self.model.predict(X_sample.reshape(1, -1))\n",
        "        y_pred_rf = self.rf_model.predict_proba(X_sample.reshape(1, -1))\n",
        "        y_pred_onehot = np.zeros(num_classes)\n",
        "        y_pred_onehot[np.argmax(y_pred_nn)] = 0.7  # Weight NN more\n",
        "        y_pred_onehot[np.argmax(y_pred_rf)] += 0.3  # Weight RF less\n",
        "        state = np.concatenate([X_sample, y_pred_onehot])\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        X_sample = self.X_data[self.current_step]\n",
        "        y_pred_nn = self.model.predict(X_sample.reshape(1, -1))\n",
        "        y_pred_rf = self.rf_model.predict_proba(X_sample.reshape(1, -1))\n",
        "        y_pred_class = np.argmax(0.7 * y_pred_nn + 0.3 * y_pred_rf[0])\n",
        "        y_true_class = self.y_true[self.current_step]\n",
        "\n",
        "        # Reward logic\n",
        "        reward = 0\n",
        "        if y_true_class != 0:  # Not BENIGN\n",
        "            if action in [0, 1]:  # Block or Throttle\n",
        "                reward = 1  # Successful mitigation\n",
        "            else:\n",
        "                reward = -2  # Strong penalty for missed attack\n",
        "        else:  # BENIGN\n",
        "            if action == 2:  # Allow\n",
        "                reward = 1  # Correctly allowed benign traffic\n",
        "            else:\n",
        "                reward = -1  # Penalty for blocking benign\n",
        "\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.X_data)\n",
        "        next_X = self.X_data[self.current_step] if not done else X_sample\n",
        "        next_y_pred_nn = self.model.predict(next_X.reshape(1, -1))\n",
        "        next_y_pred_rf = self.rf_model.predict_proba(next_X.reshape(1, -1))\n",
        "        next_y_pred_onehot = np.zeros(num_classes)\n",
        "        next_y_pred_onehot[np.argmax(next_y_pred_nn)] = 0.7\n",
        "        next_y_pred_onehot[np.argmax(next_y_pred_rf)] += 0.3\n",
        "        next_state = np.concatenate([next_X, next_y_pred_onehot])\n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "# Initialize and train RL model\n",
        "env = IoTThreatMitigationEnv(model, rf_model, X_test, y_test)\n",
        "check_env(env)\n",
        "rl_model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "rl_model.learn(total_timesteps=10000)\n",
        "rl_model.save(\"iot_mitigation_ppo\")\n",
        "print(\"RL model trained and saved as iot_mitigation_ppo\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}